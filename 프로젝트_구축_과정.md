# 📊 플레이스여행검색 Dashboard 구축 과정

## 🎯 프로젝트 개요
- **목적**: XLSX 파일 기반으로 query별 area_sc, area_cc 주간 지표를 분석하는 도구를 만들고자 함
- **기술 스택**: Node.js + Express + Chart.js를 사용함
- **접속 URL**: http://localhost:3000

---

## 🔨 구축 단계별 프롬프트 요약

### 1단계: 프로젝트 초기 설정
```
"주간지표 트래킹 웹 애플리케이션을 만들어줘. 
XLSX 파일을 업로드하면 query별로 area_sc, area_cc의 주간 증감을 분석하고 
차트로 시각화해주는 도구가 필요해."
```

**구현한 결과물:**
- Express 기반의 웹 서버를 구축했음
- 프로젝트의 기본 디렉토리 구조를 생성했음
- package.json 파일을 설정하여 필요한 의존성 패키지들을 정의했음

**세부 내용:**
- Node.js와 Express를 사용하여 백엔드 서버를 구성했음
- 프론트엔드는 HTML, CSS, JavaScript로 구성했음
- 서버는 3000번 포트에서 실행되도록 설정했음

---

### 2단계: 파일 업로드 기능
```
"XLSX 파일 업로드 기능을 추가해줘. 
파일에는 query, period, area_sc, area_cc 컬럼이 있어. 
Period 형식은 '2025-08-11~2025-08-17' 이런 식이야."
```

**구현한 결과물:**
- Multer 라이브러리를 사용하여 파일 업로드 API를 구현했음
- XLSX 라이브러리를 사용하여 엑셀 파일을 파싱하는 기능을 추가했음
- 사용자가 편리하게 파일을 업로드할 수 있도록 드래그 앤 드롭 UI를 제공했음

**세부 내용:**
- 파일은 메모리 스토리지 방식으로 처리하여 디스크 I/O를 최소화했음
- 업로드된 파일의 형식을 검증하여 XLSX 파일만 받도록 제한했음
- 업로드 진행률을 실시간으로 표시하는 프로그레스 바를 추가했음

---

### 3단계: 데이터 분석 로직
```
"같은 query와 period 조합이 중복되면:
- area_sc는 가장 큰 값만 저장
- area_cc는 모든 값을 합산
이런 로직으로 데이터를 정리해줘."
```

**구현한 결과물:**
- 중복된 데이터를 자동으로 정리하는 로직을 구현했음
- 분석된 데이터를 JSON 형식으로 저장하는 기능을 추가했음 (weekly_data.json)
- 파일 업로드 기록을 추적하여 관리하는 히스토리 기능을 만들었음

**세부 내용:**
- 동일한 query와 period 조합이 있을 경우 area_sc는 최대값을 선택하도록 했음
- area_cc는 모든 값을 합산하여 저장하도록 했음
- 각 업로드마다 시각, 파일명, 처리된 레코드 수를 기록했음

---

### 4단계: 차트 시각화
```
"Query별로 기간에 따른 area_sc, area_cc 변화를 차트로 보여줘.
선 그래프, 막대 그래프, 증감율 그래프 세 가지 타입으로 전환할 수 있게 해줘."
```

**구현한 결과물:**
- Chart.js 라이브러리를 사용하여 인터랙티브한 차트를 구현했음
- 선 그래프(line), 막대 그래프(bar), 증감율 그래프(growth) 세 가지 타입을 제공했음
- 버튼 클릭만으로 차트 타입을 실시간으로 전환할 수 있는 기능을 추가했음

**세부 내용:**
- area_sc와 area_cc 데이터를 시각적으로 비교할 수 있도록 했음
- 증감율은 이전 기간 대비 퍼센트로 계산하여 표시했음
- 차트에 마우스를 올리면 상세 수치를 확인할 수 있는 툴팁을 제공했음

---

### 5단계: 대시보드 기능
```
"전체 데이터를 한눈에 볼 수 있는 대시보드를 만들어줘.
- Top 20 쿼리 (area_sc 기준)
- 기간별 전체 트렌드
- 쿼리 검색 기능
이런 기능들이 필요해."
```

**구현한 결과물:**
- 전체 데이터를 한눈에 파악할 수 있는 대시보드 섹션을 추가했음
- area_sc 기준으로 상위 20개 쿼리를 보여주는 Top 쿼리 리스트를 만들었음
- 전체 데이터의 기간별 트렌드를 차트로 표시했음
- 쿼리명으로 실시간 검색할 수 있는 기능을 구현했음

**세부 내용:**
- Top 쿼리는 총 area_sc 값이 높은 순서대로 정렬하여 표시했음
- 검색어를 입력하면 실시간으로 쿼리 목록을 필터링했음
- 검색 결과를 드롭다운 형태로 표시하여 쉽게 선택할 수 있도록 했음

---

### 6단계: 필터 분석 기능
```
"특정 기간이나 SR Area로 필터링해서 데이터를 분석할 수 있게 해줘.
기간 범위를 선택하면 해당 기간의 데이터만 보여주고,
SR Area(otB, otL, otr)별로도 필터링할 수 있어야 해."
```

**구현한 결과물:**
- 시작일과 종료일을 선택하여 특정 기간의 데이터만 필터링하는 기능을 추가했음
- SR Area(otB, otL, otr)를 멀티로 선택할 수 있는 필터를 구현했음
- 필터링된 결과를 차트로 시각화하여 보여주는 기능을 만들었음

**세부 내용:**
- 날짜 입력은 캘린더 형태로 제공하여 사용자 편의성을 높였음
- 여러 개의 SR Area를 동시에 선택하여 비교 분석할 수 있도록 했음
- 필터 적용 시 즉시 결과가 반영되도록 구현했음

---

### 7단계: SR Area 태깅 기능
```
"각 쿼리의 기간별로 SR Area를 수동으로 입력할 수 있는 기능이 필요해.
드롭다운으로 otB, otL, otr 중 하나를 선택하면 저장되게 해줘."
```

**구현한 결과물:**
- 데이터 테이블에서 직접 SR Area를 수정할 수 있는 인라인 드롭다운 UI를 구현했음
- 선택한 SR Area 값을 즉시 서버에 저장하는 기능을 추가했음
- 페이지를 새로고침해도 입력한 데이터가 유지되도록 구현했음

**세부 내용:**
- 각 셀을 클릭하면 드롭다운 메뉴가 나타나도록 했음
- otB, otL, otr 세 가지 옵션과 빈 값(미지정)을 선택할 수 있도록 했음
- 변경 즉시 weekly_data.json 파일에 반영되도록 했음

---

### 8단계: 데이터 관리 기능
```
"업로드한 데이터를 삭제할 수 있는 기능을 추가해줘.
특정 기간의 데이터만 골라서 삭제할 수 있어야 해."
```

**구현한 결과물:**
- 특정 기간의 데이터만 선택하여 삭제할 수 있는 API를 구현했음
- 실수로 삭제하는 것을 방지하기 위해 확인 모달창을 추가했음
- 삭제 전에 자동으로 백업 파일을 생성하는 기능을 만들었음

**세부 내용:**
- 기간을 입력받아 해당 기간의 모든 데이터를 제거하도록 했음
- 삭제 시 weekly_data.json.backup 파일을 생성하여 복구 가능하도록 했음
- 삭제 후 업로드 히스토리도 함께 업데이트되도록 했음

---

### 9단계: UI/UX 개선
```
"전체적인 UI를 더 깔끔하고 직관적으로 개선해줘.
- 모던한 디자인
- 반응형 레이아웃
- 로딩 인디케이터
- 에러 처리 및 사용자 피드백"
```

**구현한 결과물:**
- 모던하고 깔끔한 느낌의 CSS 스타일을 적용했음
- 파일 업로드 시 진행 상황을 보여주는 프로그레스 바를 추가했음
- 사용자 액션에 대한 피드백을 제공하는 Toast 알림을 구현했음
- 오류 발생 시 친절한 메시지를 표시하는 에러 핸들링을 추가했음

**세부 내용:**
- 컬러 스킴은 파란색 계열로 통일하여 일관성을 유지했음
- 버튼 호버 효과와 트랜지션 애니메이션을 추가했음
- 모바일 환경에서도 잘 보이도록 반응형 레이아웃을 적용했음

---

## 📁 주요 파일 구조

```
weekly-tracker/
├── server.js              # Express 서버 및 API 엔드포인트들을 정의한 파일
├── package.json           # 프로젝트 의존성과 스크립트를 관리하는 파일
├── data/
│   ├── weekly_data.json   # 분석된 모든 쿼리 데이터를 저장하는 파일
│   └── upload_history.json # 파일 업로드 기록을 저장하는 파일
└── public/
    ├── index.html         # 메인 웹 페이지 HTML 파일
    ├── script.js          # 클라이언트 측 JavaScript 로직을 담은 파일
    └── style.css          # 전체 페이지 스타일을 정의한 CSS 파일
```

---

## 🚀 핵심 기능

### 1. 파일 업로드
- 사용자가 드래그 앤 드롭으로 편리하게 파일을 업로드할 수 있도록 지원함
- XLSX 형식의 엑셀 파일을 파싱하여 데이터를 추출함
- 업로드 진행 상황을 실시간으로 표시하여 사용자 경험을 개선함

### 2. 데이터 분석
- 각 쿼리별로 주간 지표를 자동으로 집계하여 저장함
- 중복된 데이터는 설정한 규칙에 따라 자동으로 처리함
- 이전 기간 대비 증감율을 자동으로 계산하여 제공함

### 3. 시각화
- Chart.js 라이브러리를 활용하여 인터랙티브한 차트를 제공함
- 선 그래프, 막대 그래프, 증감율 그래프 중 원하는 형태로 전환할 수 있음
- 기간별 트렌드를 한눈에 파악할 수 있도록 시각화함

### 4. 필터링
- 원하는 기간 범위를 선택하여 해당 데이터만 조회할 수 있음
- SR Area별로 데이터를 필터링하여 분석할 수 있음
- 쿼리명으로 실시간 검색하여 빠르게 찾을 수 있음

### 5. 데이터 관리
- 모든 파일 업로드 기록을 추적하여 이력을 관리함
- 특정 기간의 데이터를 선택하여 삭제할 수 있음
- 데이터 삭제 시 자동으로 백업 파일을 생성하여 복구 가능하도록 함

---

## 🔧 최근 개선 사항

### 데이터 처리 개선
```
"'top 20000 query.xlsx' 파일의 value 열을 JSON 파싱해서 
각 키-값을 별도 열로 분리해줘."
```

**구현한 내용:**
- value 열의 JSON 데이터를 파싱하는 Python 스크립트를 작성했음
- 30,000개의 쿼리 데이터를 성공적으로 처리했음
- travel_city, travel_cityCode 등 10개의 새로운 컬럼을 추가했음

**세부 내용:**
- JSON 구조를 분석하여 travel, travelReco 등의 키를 추출했음
- 각 키의 하위 값들을 개별 컬럼으로 분리했음
- 분리된 데이터를 새로운 Excel 파일로 저장했음

### 버그 수정
```
"2025-09-29~2025-10-05.xlsx 파일의 period 열에서 
'2025-09-29~2025-10--05' (하이픈 중복)를 
'2025-09-29~2025-10-05'로 일괄 수정해줘."
```

**구현한 내용:**
- period 열의 잘못된 하이픈 중복 문제를 발견하고 수정했음
- 총 57,724개 행의 데이터를 일괄적으로 수정했음
- 원본 파일은 백업 파일로 보존하고 수정된 파일을 저장했음

**세부 내용:**
- Python pandas를 사용하여 일괄 수정 스크립트를 작성했음
- 수정 전후를 비교하여 정확성을 검증했음
- 백업 파일을 자동 생성하여 안전하게 처리했음

---

## 💡 사용 방법

### 서버 시작하기
```bash
cd weekly-tracker
npm install
npm start
```
→ 브라우저에서 http://localhost:3000 으로 접속하면 됨

### 데이터 업로드하기
1. 화면 상단의 "새 파일 업로드" 버튼을 클릭함
2. XLSX 파일을 선택하거나 드래그하여 업로드함
3. 시스템이 자동으로 데이터를 분석하고 저장함

### 데이터 분석하기
1. "기존 데이터 조회" 메뉴를 선택하면 쿼리별 상세 분석을 볼 수 있음
2. "대시보드" 메뉴를 선택하면 전체 현황을 한눈에 파악할 수 있음
3. "필터 분석" 메뉴를 선택하면 조건별로 데이터를 필터링하여 분석할 수 있음

---

## 📊 데이터 저장 위치

### 서버에 저장되는 데이터
```
/Users/user/cursor/weekly-tracker/data/
├── weekly_data.json (243MB) - 분석된 모든 쿼리 데이터가 저장되어 있음
└── upload_history.json (7.2KB) - 파일 업로드 기록이 저장되어 있음
```

### 원본 파일 위치
```
/Users/user/Documents/쿼리별 주간지표/
└── *.xlsx - 업로드했던 원본 엑셀 파일들이 보관되어 있음
```

---

## 🎓 개발하면서 배운 점과 팁

### 성능 최적화 관련
- 대용량 파일은 디스크에 저장하지 않고 메모리에만 올려서 처리하여 디스크 I/O를 최소화했음
- JSON 데이터를 파싱할 때는 스트리밍 방식을 고려하여 메모리 사용량을 줄이는 것이 좋음
- 차트에 표시되는 데이터가 많을 경우 페이지네이션을 적용하여 렌더링 속도를 개선할 수 있음

### 에러 처리 관련
- 업로드되는 파일의 형식을 미리 검증하여 잘못된 파일을 걸러내도록 했음
- 중복 데이터는 자동으로 정리하는 로직을 구현하여 데이터 품질을 유지했음
- 오류 발생 시 사용자가 이해하기 쉬운 메시지를 표시하여 혼란을 줄였음

### 유지보수 관련
- 중요한 데이터는 항상 자동으로 백업 파일을 생성하도록 했음
- 모든 파일 업로드 이력을 기록하여 나중에 추적할 수 있도록 했음
- 서버 콘솔에 명확한 로그를 출력하여 문제 발생 시 원인을 빠르게 파악할 수 있도록 했음

---

## 📞 문의 및 지원
- 서버 접속 URL: http://localhost:3000
- 데이터 저장 위치: `weekly-tracker/data/` 디렉토리
- 로그 확인 방법: 서버를 실행한 터미널의 콘솔 출력을 참고하면 됨

---

**문서 작성일**: 2025-11-05  
**현재 버전**: 1.0  
**개발에 소요된 기간**: 약 2-3주  
**현재까지 처리한 데이터**: 45개 파일, 총 2.1M+ 레코드
