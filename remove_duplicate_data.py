#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import subprocess
import os
from datetime import datetime

def remove_duplicate_data():
    """ì¤‘ë³µëœ 2025-04-07~2025-04-13 ë°ì´í„°ë¥¼ ì œê±°"""
    
    print("ğŸ” ì¤‘ë³µ ë°ì´í„° í™•ì¸ ì¤‘...")
    
    # 1. í˜„ì¬ ë°ì´í„° ë¡œë“œ
    try:
        result = subprocess.run(['curl', '-s', 'http://localhost:3000/api/data'], 
                              capture_output=True, text=True, check=True)
        data = json.loads(result.stdout)
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 2. ì—…ë¡œë“œ íˆìŠ¤í† ë¦¬ ë¡œë“œ
    try:
        result = subprocess.run(['curl', '-s', 'http://localhost:3000/api/history'], 
                              capture_output=True, text=True, check=True)
        history_response = json.loads(result.stdout)
        history = history_response['history'] if 'history' in history_response else history_response
    except Exception as e:
        print(f"âŒ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 3. 2025-04-07~2025-04-13 íŒŒì¼ì˜ ì—…ë¡œë“œ ê¸°ë¡ ì°¾ê¸°
    target_file = "2025-04-07~2025-04-13.xlsx"
    duplicate_uploads = [h for h in history if h['filename'] == target_file]
    
    print(f"ğŸ“ {target_file} ì—…ë¡œë“œ ê¸°ë¡: {len(duplicate_uploads)}ê°œ")
    for i, upload in enumerate(duplicate_uploads):
        print(f"  {i+1}. {upload['timestamp']} - {upload['recordsAdded']:,}ê°œ ë ˆì½”ë“œ")
    
    if len(duplicate_uploads) < 2:
        print("âœ… ì¤‘ë³µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 4. ê°€ì¥ ìµœê·¼ ì—…ë¡œë“œë¥¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    duplicate_uploads.sort(key=lambda x: x['timestamp'])
    
    # ê°€ì¥ ì˜¤ë˜ëœ ì—…ë¡œë“œ ì œê±° (ì²« ë²ˆì§¸ ì—…ë¡œë“œ)
    upload_to_remove = duplicate_uploads[0]
    print(f"ğŸ—‘ï¸  ì œê±°í•  ì—…ë¡œë“œ: {upload_to_remove['timestamp']}")
    
    # 5. ë°ì´í„°ì—ì„œ í•´ë‹¹ ê¸°ê°„ì˜ ë°ì´í„° ì œê±°
    period_to_remove = "2025-04-07~2025-04-13"
    removed_queries = 0
    
    # ê° queryì—ì„œ í•´ë‹¹ ê¸°ê°„ì˜ ë°ì´í„° ì œê±°
    for query, query_data in data['data'].items():
        if isinstance(query_data, dict) and 'periods' in query_data:
            periods = query_data['periods']
            if period_to_remove in periods:
                # í•´ë‹¹ ê¸°ê°„ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                period_index = periods.index(period_to_remove)
                
                # í•´ë‹¹ ê¸°ê°„ì˜ ë°ì´í„° ì œê±°
                if 'areaSc' in query_data and len(query_data['areaSc']) > period_index:
                    query_data['areaSc'].pop(period_index)
                if 'areaCc' in query_data and len(query_data['areaCc']) > period_index:
                    query_data['areaCc'].pop(period_index)
                if 'srArea' in query_data and len(query_data['srArea']) > period_index:
                    query_data['srArea'].pop(period_index)
                if 'scGrowthRates' in query_data and len(query_data['scGrowthRates']) > period_index:
                    query_data['scGrowthRates'].pop(period_index)
                if 'ccGrowthRates' in query_data and len(query_data['ccGrowthRates']) > period_index:
                    query_data['ccGrowthRates'].pop(period_index)
                
                # periodsì—ì„œë„ ì œê±°
                periods.pop(period_index)
                removed_queries += 1
    
    # 6. ì—…ë¡œë“œ íˆìŠ¤í† ë¦¬ì—ì„œë„ ì œê±°
    history = [h for h in history if h['timestamp'] != upload_to_remove['timestamp']]
    
    # 7. ë°ì´í„° ì €ì¥
    data_file = 'data/weekly_data.json'
    history_file = 'data/upload_history.json'
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # ë°ì´í„° ì €ì¥
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì¤‘ë³µ ë°ì´í„° ì œê±° ì™„ë£Œ!")
    print(f"ğŸ“Š ì œê±°ëœ ì¿¼ë¦¬ ìˆ˜: {removed_queries:,}ê°œ")
    print(f"ğŸ“ ì œê±°ëœ ì—…ë¡œë“œ: {upload_to_remove['timestamp']}")
    print(f"ğŸ’¾ ë°ì´í„° íŒŒì¼ ì—…ë°ì´íŠ¸: {data_file}")
    print(f"ğŸ’¾ íˆìŠ¤í† ë¦¬ íŒŒì¼ ì—…ë°ì´íŠ¸: {history_file}")
    
    # 8. ì„œë²„ ì¬ì‹œì‘ í•„ìš” ì•ˆë‚´
    print("\nğŸ”„ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì•¼ ë³€ê²½ì‚¬í•­ì´ ì ìš©ë©ë‹ˆë‹¤.")
    print("   ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
    
    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
    response = input().strip().lower()
    if response == 'y':
        print("ğŸ”„ ì„œë²„ ì¬ì‹œì‘ ì¤‘...")
        # ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        subprocess.run(['pkill', '-f', 'node server.js'], capture_output=True)
        # ì„œë²„ ì¬ì‹œì‘
        subprocess.Popen(['node', 'server.js'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print("âœ… ì„œë²„ ì¬ì‹œì‘ ì™„ë£Œ!")

if __name__ == "__main__":
    remove_duplicate_data()
